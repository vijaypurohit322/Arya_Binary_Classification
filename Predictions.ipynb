{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86470288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score,classification_report,confusion_matrix,log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8c248",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defe8b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.200</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>16.304</td>\n",
       "      <td>148</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.440</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.338</td>\n",
       "      <td>123</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1    X2    X3     X4    X5    X6    X7    X8   X9   X10  ...  X49  \\\n",
       "0  0.00  0.00  4.34   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "1  0.00  0.56  0.56   0.00  1.12  0.56  2.25  0.00  0.0  0.56  ...  0.0   \n",
       "2  0.00  0.00  0.00   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "3  0.64  0.00  0.64   0.00  1.93  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "4  0.58  0.00  0.00  35.46  0.58  0.00  0.58  0.58  0.0  0.00  ...  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56  X57  Y  \n",
       "0  0.000  0.0  1.342  0.000  0.000   1.200    2   12  0  \n",
       "1  0.083  0.0  0.503  0.000  0.083  16.304  148  375  1  \n",
       "2  0.000  0.0  0.000  0.000  0.000   1.000    1    5  0  \n",
       "3  0.000  0.0  0.462  0.370  0.000   2.440   22  122  1  \n",
       "4  0.000  0.0  0.239  0.239  0.000   3.338  123  207  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the train data\n",
    "df = pd.read_csv('data/training_set.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9a9484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.342</td>\n",
       "      <td>47</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.375</td>\n",
       "      <td>168</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.153</td>\n",
       "      <td>5.891</td>\n",
       "      <td>193</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.015</td>\n",
       "      <td>8.550</td>\n",
       "      <td>669</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.781</td>\n",
       "      <td>32</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1   X2    X3   X4    X5    X6    X7    X8    X9   X10  ...  X48  X49  \\\n",
       "0  0.70  0.0  0.70  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "1  0.00  0.0  0.84  0.0  0.84  0.00  0.84  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "2  0.46  0.3  0.46  0.0  0.05  0.12  0.05  0.28  0.43  0.74  ...  0.0  0.0   \n",
       "3  0.10  0.2  1.01  0.0  0.80  0.80  0.50  0.00  0.80  0.10  ...  0.0  0.0   \n",
       "4  0.00  0.0  0.72  0.0  0.72  0.00  0.72  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56   X57  \n",
       "0  0.000  0.0  0.105  0.000  0.000   2.342   47    89  \n",
       "1  0.388  0.0  0.776  0.129  0.000  10.375  168   249  \n",
       "2  0.065  0.0  0.325  0.756  0.153   5.891  193  3040  \n",
       "3  0.110  0.0  0.490  0.158  0.015   8.550  669  1351  \n",
       "4  0.364  0.0  0.729  0.121  0.000   7.781   32   249  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Test data\n",
    "test_data = pd.read_csv('data/test_set.csv',index_col=0)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b10eb7",
   "metadata": {},
   "source": [
    "# Splitting into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbdebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Y'],axis=1)\n",
    "y = df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640f286",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1096e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6561a",
   "metadata": {},
   "source": [
    "# Using RandomForest Classifier for feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31d9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(100, max_depth=None, n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d63636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the features with their respect to feature importances\n",
    "fi = sorted(zip(X.columns,feature_importance),key=lambda x: x[1], reverse=True)\n",
    "# Extracting Top 30 features\n",
    "top_features = [x[0] for x in fi[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11c867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top features from data\n",
    "X_train_dash = X_train[top_features]\n",
    "X_test_dash = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4057e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top features for test data\n",
    "test_data_dash = test_data[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d60e6",
   "metadata": {},
   "source": [
    "# Normalizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d886666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler to normalize our data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_dash)\n",
    "\n",
    "# transform\n",
    "X_train_dash = pd.DataFrame(scaler.transform(X_train_dash),columns=X_train_dash.columns)\n",
    "X_test_dash = pd.DataFrame(scaler.transform(X_test_dash),columns=X_test_dash.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1c1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler to normalize our test data\n",
    "test_data_dash = pd.DataFrame(scaler.transform(test_data_dash),columns=test_data_dash.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a1f43",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fa985",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c478d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Logloss for the model -> 0.14307064312877005\n",
      "Validation Logloss for the model -> 0.15659173915300834\n",
      "--------------------------------------------------\n",
      "Train AUC Score for the model -> 0.988952325096344\n",
      "Validation AUC Score for the model -> 0.9873889936567805\n"
     ]
    }
   ],
   "source": [
    "# Using Xgboost\n",
    "classifier = XGBClassifier(n_estimators=500,\n",
    "                           max_depth=5,\n",
    "                           learning_rate=0.15,\n",
    "                           colsample_bytree=1,\n",
    "                           subsample=1,\n",
    "                           reg_alpha = 0.3,\n",
    "                           gamma=10,\n",
    "                           n_jobs=-1,\n",
    "                           eval_metric='logloss',\n",
    "                           use_label_encoder=False)\n",
    "\n",
    "classifier.fit(X_train_dash, y_train)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train_dash)\n",
    "y_train_prob = classifier.predict_proba(X_train_dash)[:,1]\n",
    "y_val_pred = classifier.predict(X_test_dash)\n",
    "y_val_prob = classifier.predict_proba(X_test_dash)[:,1]\n",
    "\n",
    "\n",
    "# Calculating logloss score for our model\n",
    "print(f'Train Logloss for the model -> {log_loss(y_train,y_train_prob)}')\n",
    "print(f'Validation Logloss for the model -> {log_loss(y_val,y_val_prob)}')\n",
    "\n",
    "print('-'*50)\n",
    "# Calculating the AUC score for our model\n",
    "print(f'Train AUC Score for the model -> {roc_auc_score(y_train, y_train_prob)}')\n",
    "print(f'Validation AUC Score for the model -> {roc_auc_score(y_val, y_val_prob)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874b210",
   "metadata": {},
   "source": [
    "# Predictions on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b5d401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
      " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = classifier.predict(test_data_dash)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bb9ec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04271409 0.9832941  0.9784445  0.9707199  0.9832941  0.25391334\n",
      " 0.9813176  0.7845885  0.9663585  0.98225135 0.01083483 0.06842812\n",
      " 0.21198574 0.01833995 0.05076724 0.06760822 0.02539814 0.16887471\n",
      " 0.9663585  0.02184766 0.97454685 0.9735979  0.9714138  0.01123517\n",
      " 0.72341365 0.06946245 0.11394848 0.85844535 0.06946245 0.01146475\n",
      " 0.8965603  0.05694852 0.01833995 0.05694852 0.01179442 0.02831707\n",
      " 0.46751907 0.02849386 0.61065644 0.47207215 0.9766541  0.13998061\n",
      " 0.16887471 0.13001329 0.05054418 0.9363092  0.06672557 0.02184766\n",
      " 0.05097714 0.97791696 0.85415566 0.11893313 0.0121338  0.94125766\n",
      " 0.97723126 0.9777154  0.02660294 0.02445995 0.46172813 0.95680577\n",
      " 0.9787588  0.143574   0.287279   0.18906485 0.40167016 0.9629246\n",
      " 0.89245486 0.7384918  0.1506621  0.05694852 0.02674611 0.8565982\n",
      " 0.05694852 0.59294015 0.89968246 0.9627089  0.10170943 0.01059248\n",
      " 0.31736988 0.05694852 0.07162196 0.9541347  0.92554635 0.9702992\n",
      " 0.05694852 0.01833995 0.05704243 0.05694852 0.96765643 0.01336795\n",
      " 0.05694852 0.13641086 0.02259401 0.0936114  0.02791856 0.8139128\n",
      " 0.7184958  0.09158579 0.9789259  0.7903328  0.01083483 0.03155415\n",
      " 0.23756854 0.69034326 0.98016465 0.04862737 0.28910014 0.03287483\n",
      " 0.05222667 0.9586776  0.03879023 0.9149733  0.04025516 0.9673469\n",
      " 0.9833739  0.05686988 0.28910014 0.05694852 0.06297929 0.02874262\n",
      " 0.25062943 0.02414625 0.0334662  0.15264778 0.08752626 0.03192753\n",
      " 0.050129   0.45314533 0.01833995 0.9598408  0.560933   0.06196877\n",
      " 0.02868381 0.1085977  0.9833739  0.05694852 0.07162196 0.61096245\n",
      " 0.06605946 0.98276144 0.06946245 0.03417914 0.07639471 0.02906989\n",
      " 0.01336795 0.9348462  0.94649446 0.09777354 0.8069485  0.9612037\n",
      " 0.01250797 0.9725815  0.9762432  0.05333938 0.07026761 0.02791071\n",
      " 0.97133607 0.05694852 0.00901831 0.4063348  0.97645426 0.05065723\n",
      " 0.01833995 0.03047892 0.8299245  0.02616533 0.05526939 0.05694852\n",
      " 0.11661711 0.01174072 0.03805759 0.9826346  0.25261268 0.9736388\n",
      " 0.0257285  0.14101902 0.98188305 0.05694852 0.9755206  0.9173858\n",
      " 0.03155415 0.04883618 0.01156856 0.96877635 0.13627918 0.04562606\n",
      " 0.944027   0.35260144 0.06533603 0.9838626  0.8101116  0.04023759\n",
      " 0.96877635 0.7318829  0.7192623  0.836255   0.11453047 0.05694852\n",
      " 0.25509176 0.96734124 0.05694852 0.95336545 0.07692669 0.00940218\n",
      " 0.10348875 0.9456245  0.8476251  0.04668595 0.9801537  0.05185027\n",
      " 0.34822023 0.97741926 0.02841041 0.01698961 0.04257745 0.10313233\n",
      " 0.05264526 0.14101902 0.01336795 0.9842365  0.02099571 0.97901183\n",
      " 0.03564263 0.25495592 0.05694852 0.85481954 0.95930237 0.05694852\n",
      " 0.05694852 0.00901831 0.09471729 0.03004854 0.9194591  0.01259425\n",
      " 0.05694852 0.19252574 0.9755206  0.0231589  0.04129481 0.01013614\n",
      " 0.04266191 0.08370114 0.04258916 0.02445995 0.04889994 0.06885609\n",
      " 0.0984752  0.02559809 0.03980103 0.97675973 0.06946245 0.94084436\n",
      " 0.04492912 0.88042736 0.02949058 0.04257745 0.63453245 0.6134362\n",
      " 0.9711774  0.96437997 0.02334583 0.01833995 0.9523996  0.05694852\n",
      " 0.9663585  0.02699855 0.05694852 0.16887471 0.62627155 0.02703283\n",
      " 0.01833995 0.9823199  0.9784445  0.05249146 0.9806316  0.6994321\n",
      " 0.5229354  0.07437707 0.01986749 0.01083483 0.00952571 0.01833995\n",
      " 0.8966864  0.19811118 0.9377671  0.10617656 0.05694852 0.9536988\n",
      " 0.06668194 0.06196877 0.64446485 0.03783979 0.02864859 0.85415566\n",
      " 0.920662   0.01013614 0.43162778 0.03762799 0.05694852 0.9697753\n",
      " 0.01960211 0.96477616 0.00901831 0.02433812 0.2847046  0.02449575\n",
      " 0.02445995 0.97569215 0.01833995 0.9748215  0.7132306  0.96953124\n",
      " 0.06256244 0.0334662  0.82304406 0.02579976 0.02044188 0.09637285\n",
      " 0.02579976 0.03374705 0.08772345 0.16887471 0.07305223 0.98016465\n",
      " 0.01833995 0.906204   0.03893161 0.9707199  0.03750907 0.01833995\n",
      " 0.02390097 0.7846966  0.8008577  0.05694852 0.6533469  0.7412745\n",
      " 0.0903889  0.27041432 0.07710741 0.07703568 0.23756854 0.03252376\n",
      " 0.05694852 0.97694206 0.04011266 0.05694852 0.7227121  0.02264535\n",
      " 0.21244033 0.38679376 0.02864859 0.9711774  0.10332419 0.01501141\n",
      " 0.9725921  0.02264535 0.02264535 0.07703568 0.5011988  0.37626573\n",
      " 0.05694852 0.04561852 0.04818144 0.05694852 0.06367891 0.03120859\n",
      " 0.965855   0.02264535 0.48528847 0.95227355 0.92454016 0.923909\n",
      " 0.11568937 0.00950385 0.7605361  0.02449575 0.08170133 0.93913704\n",
      " 0.9845889  0.95185256 0.9777154  0.9842365  0.02726094 0.04955237\n",
      " 0.01833995 0.02253779 0.9743299  0.02723449 0.02571009 0.9842365\n",
      " 0.8881187  0.98563945 0.37577793 0.97734857 0.97363645 0.97758865\n",
      " 0.04073763 0.06568816 0.01833995 0.9696935  0.8939446  0.9671609\n",
      " 0.94228727 0.95757324 0.94971365 0.9246674  0.06672204 0.9592478\n",
      " 0.05249146 0.02238867 0.02539814 0.983548   0.9745796  0.03010974\n",
      " 0.02309098 0.04136892 0.02445995 0.02914707 0.05694852 0.96355695\n",
      " 0.959792   0.90199137 0.10332419 0.23756854 0.9330102  0.04562606\n",
      " 0.06258029 0.14101902 0.13666537 0.11568937 0.94228727 0.08567747\n",
      " 0.02563714 0.07947182 0.06352586 0.02669631 0.05054418 0.9749477\n",
      " 0.983548   0.15359919 0.25062943 0.9719971  0.97830015 0.09316813\n",
      " 0.03750907 0.10332419 0.01804862 0.9766541  0.9554911  0.1053963\n",
      " 0.57596844 0.94671565 0.96918386 0.02728396 0.03651969 0.9580286\n",
      " 0.09134679 0.1018995  0.97303075 0.9690615  0.01083483 0.06946245\n",
      " 0.9314762  0.85481954 0.02264535 0.9017493  0.09471729 0.01833995\n",
      " 0.95879227 0.9286218  0.01075871 0.27457964 0.98025906 0.02868381\n",
      " 0.35598233 0.9432353  0.9816664  0.9408279  0.98108554 0.9246678\n",
      " 0.01833995 0.0679611  0.98440176 0.02173117 0.039437   0.07375643\n",
      " 0.02158154 0.13445698 0.02001681 0.05694852 0.02301596 0.9438791\n",
      " 0.02791856 0.16073321 0.9229461  0.9627383  0.92583287 0.5035856\n",
      " 0.94153774 0.06833237 0.5070464  0.9764074  0.1172381  0.0458223\n",
      " 0.97539073 0.29077384 0.894238   0.05694852 0.97887427 0.57473934\n",
      " 0.2787625  0.6562523  0.01833995 0.95794636 0.05694852 0.1392267\n",
      " 0.015338   0.9542093  0.01997692 0.0463746  0.01432188 0.00834846\n",
      " 0.9614432  0.62806225 0.01986577 0.02290688 0.017251   0.02268918\n",
      " 0.8785694  0.05694852 0.12817484 0.17621545 0.06674881 0.02054218\n",
      " 0.05694852 0.9653742  0.10008064 0.02877061 0.030652   0.01059248\n",
      " 0.06946245 0.07628169 0.00996987 0.06937028 0.07162196 0.02955559\n",
      " 0.12809299 0.01422281 0.02427406 0.05694852 0.07710741 0.17705749\n",
      " 0.9460988  0.01833995 0.02264535 0.06946245 0.89968246 0.10712374\n",
      " 0.8681031  0.85285556 0.13940491 0.9627089  0.12793323 0.11859224\n",
      " 0.08274063 0.9084966  0.02259401 0.05694852 0.05694852 0.79923666\n",
      " 0.01833995 0.8927692  0.7935088  0.98077273 0.01083483 0.0168468\n",
      " 0.2702724  0.01833995 0.01500017 0.01833995 0.97356397 0.9673469\n",
      " 0.88852715 0.48724258 0.25062943 0.01833995 0.966412   0.9464771\n",
      " 0.04728486 0.21209691 0.95330864 0.17488362 0.89473075 0.94990087\n",
      " 0.923909   0.9807273  0.03888566 0.8889253  0.01833995 0.01833995\n",
      " 0.01081626 0.8340356  0.95530385 0.02001681 0.983548   0.03262186\n",
      " 0.03318744 0.08844394 0.01259425 0.9734538  0.95686483 0.95899016\n",
      " 0.05694852 0.01242137 0.12471695 0.04586804 0.9416476  0.956545\n",
      " 0.02112984 0.97356397 0.97118616 0.05694852 0.0696474  0.0163953\n",
      " 0.9526171  0.9807242  0.9365168  0.01048311 0.8672233  0.45799625\n",
      " 0.08305825 0.03267449 0.9558372  0.01013614 0.05686988 0.9786744\n",
      " 0.02452727 0.7720628  0.9546412  0.01886424 0.9523996  0.0170626\n",
      " 0.4782152  0.06533603 0.9824127  0.03593405 0.01923425 0.07162196\n",
      " 0.98009986 0.06484275 0.949853   0.01336795 0.028894   0.02107713\n",
      " 0.1684417  0.04815785 0.89291865 0.96657884 0.8081485  0.98200166\n",
      " 0.97908896 0.9403918  0.93087757 0.9622182  0.05694852 0.0206879\n",
      " 0.19980258 0.17705749 0.9490806  0.9291761  0.8681031  0.97388965\n",
      " 0.97435117 0.08214933 0.944455   0.57752544 0.02248904 0.50055885\n",
      " 0.02571009 0.09825117 0.07775675 0.8895298  0.9274375  0.01013614\n",
      " 0.97739947 0.01009628 0.01989661 0.01716321 0.02259401 0.0219332\n",
      " 0.03750907]\n"
     ]
    }
   ],
   "source": [
    "y_test_prob = classifier.predict_proba(test_data_dash)[:,1]\n",
    "print(y_test_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
